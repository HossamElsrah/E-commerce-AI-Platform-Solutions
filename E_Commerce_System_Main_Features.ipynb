{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12762370,
          "sourceType": "datasetVersion",
          "datasetId": 8067870
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "E_Commerce System Main Features",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "0_UIQ2WskRwE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "hossamelsrah_e_commercee_books_path = kagglehub.dataset_download('hossamelsrah/e-commercee-books')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lnrb6NvYkRwI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ecommerce Assistant Feature"
      ],
      "metadata": {
        "id": "yfeg_IEekRwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "VYVkhnCpkRwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain accelerate chromadb faiss-cpu sentence-transformers transformers langchain-huggingface langchain_experimental\n",
        "!pip install opendatasets"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:06:03.496676Z",
          "iopub.execute_input": "2025-08-14T02:06:03.497186Z",
          "iopub.status.idle": "2025-08-14T02:07:57.774796Z",
          "shell.execute_reply.started": "2025-08-14T02:06:03.497161Z",
          "shell.execute_reply": "2025-08-14T02:07:57.774002Z"
        },
        "id": "yC4csAu4kRwL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download('https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:23.495511Z",
          "iopub.execute_input": "2025-08-14T01:42:23.495738Z",
          "iopub.status.idle": "2025-08-14T01:42:37.751598Z",
          "shell.execute_reply.started": "2025-08-14T01:42:23.495717Z",
          "shell.execute_reply": "2025-08-14T01:42:37.751Z"
        },
        "id": "DqPcueMFkRwL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "orders = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_orders_dataset.csv')\n",
        "order_items = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_products_dataset.csv')\n",
        "products = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_order_items_dataset.csv')\n",
        "order_payments = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_order_payments_dataset.csv')\n",
        "reviews = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_order_reviews_dataset.csv')\n",
        "sellers = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_sellers_dataset.csv')\n",
        "geolocation = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_geolocation_dataset.csv')\n",
        "customers = pd.read_csv('/kaggle/working/brazilian-ecommerce/olist_customers_dataset.csv')\n",
        "product_category = pd.read_csv('/kaggle/working/brazilian-ecommerce/product_category_name_translation.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:40.563107Z",
          "iopub.execute_input": "2025-08-14T01:42:40.56397Z",
          "iopub.status.idle": "2025-08-14T01:42:43.066558Z",
          "shell.execute_reply.started": "2025-08-14T01:42:40.563935Z",
          "shell.execute_reply": "2025-08-14T01:42:43.065913Z"
        },
        "id": "YhaAe9CDkRwM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Explorition & Preprocessing"
      ],
      "metadata": {
        "id": "5CzsLu9AkRwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = orders.merge(products, on='order_id', how='left')\n",
        "merged_data = merged_data.merge(order_items[['product_id', 'product_category_name']], on='product_id', how='left')\n",
        "merged_data = merged_data.merge(order_payments, on='order_id', how='left')\n",
        "merged_data = merged_data.merge(reviews, on='order_id', how='left')\n",
        "merged_data = merged_data.merge(sellers, on='seller_id', how='left')\n",
        "merged_data = merged_data.merge(customers, on='customer_id', how='left')\n",
        "merged_data = merged_data.merge(product_category, on='product_category_name', how='left')\n",
        "display(merged_data.head().T)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:43.067808Z",
          "iopub.execute_input": "2025-08-14T01:42:43.068087Z",
          "iopub.status.idle": "2025-08-14T01:42:44.271047Z",
          "shell.execute_reply.started": "2025-08-14T01:42:43.068068Z",
          "shell.execute_reply": "2025-08-14T01:42:44.270198Z"
        },
        "id": "mti0_bjjkRwN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:44.271891Z",
          "iopub.execute_input": "2025-08-14T01:42:44.272246Z",
          "iopub.status.idle": "2025-08-14T01:42:44.452337Z",
          "shell.execute_reply.started": "2025-08-14T01:42:44.272218Z",
          "shell.execute_reply": "2025-08-14T01:42:44.451507Z"
        },
        "id": "agd_qwftkRwN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data=merged_data.drop(columns=[\"order_id\",\"customer_id\",\"order_approved_at\",\"order_delivered_carrier_date\",\"order_item_id\"\n",
        ",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"payment_sequential\"\n",
        ",\"payment_installments\",\"review_id\",\"review_creation_date\",\"review_answer_timestamp\"\n",
        ",\"seller_zip_code_prefix\",\"seller_state\",\"seller_city\"\n",
        ",\"customer_unique_id\",\"customer_zip_code_prefix\",\"review_comment_title\",\"review_comment_message\",\"order_purchase_timestamp\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:45.545034Z",
          "iopub.execute_input": "2025-08-14T01:42:45.545309Z",
          "iopub.status.idle": "2025-08-14T01:42:45.576639Z",
          "shell.execute_reply.started": "2025-08-14T01:42:45.545287Z",
          "shell.execute_reply": "2025-08-14T01:42:45.576033Z"
        },
        "id": "vZjOXJCGkRwO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:45.577608Z",
          "iopub.execute_input": "2025-08-14T01:42:45.577824Z",
          "iopub.status.idle": "2025-08-14T01:42:45.626695Z",
          "shell.execute_reply.started": "2025-08-14T01:42:45.577806Z",
          "shell.execute_reply": "2025-08-14T01:42:45.625965Z"
        },
        "id": "4PSxkytzkRwO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data[\"product_category_name_english\"] = merged_data[\"product_category_name_english\"].fillna(merged_data[\"product_category_name_english\"].mode()[0])\n",
        "merged_data['payment_type'] = merged_data['payment_type'].fillna(merged_data['payment_type'].mode()[0])\n",
        "\n",
        "# For numerical columns, fill with mean\n",
        "for col in ['review_score', 'payment_value', 'freight_value', 'price','delivery_time_difference']:\n",
        "     if col in merged_data.columns:\n",
        "        merged_data[col] = merged_data[col].fillna(merged_data[col].mean())\n",
        "\n",
        "merged_data.drop(columns=[\"order_estimated_delivery_date\",\"order_delivered_customer_date\",\"product_category_name\"],inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:47.208829Z",
          "iopub.execute_input": "2025-08-14T01:42:47.20948Z",
          "iopub.status.idle": "2025-08-14T01:42:47.258092Z",
          "shell.execute_reply.started": "2025-08-14T01:42:47.209433Z",
          "shell.execute_reply": "2025-08-14T01:42:47.257237Z"
        },
        "id": "fXW6A-NNkRwO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.duplicated().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:47.703792Z",
          "iopub.execute_input": "2025-08-14T01:42:47.704502Z",
          "iopub.status.idle": "2025-08-14T01:42:48.077928Z",
          "shell.execute_reply.started": "2025-08-14T01:42:47.704475Z",
          "shell.execute_reply": "2025-08-14T01:42:48.077306Z"
        },
        "id": "9rgn5AuYkRwO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:48.174579Z",
          "iopub.execute_input": "2025-08-14T01:42:48.175166Z",
          "iopub.status.idle": "2025-08-14T01:42:48.235075Z",
          "shell.execute_reply.started": "2025-08-14T01:42:48.175142Z",
          "shell.execute_reply": "2025-08-14T01:42:48.234505Z"
        },
        "id": "ncx07ihukRwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.sample()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:48.690442Z",
          "iopub.execute_input": "2025-08-14T01:42:48.69107Z",
          "iopub.status.idle": "2025-08-14T01:42:48.703896Z",
          "shell.execute_reply.started": "2025-08-14T01:42:48.691047Z",
          "shell.execute_reply": "2025-08-14T01:42:48.703273Z"
        },
        "id": "5-JWZF8ikRwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = merged_data.rename(columns={\n",
        "    'product_category_name_english': 'category',\n",
        "    'review_score': 'review',\n",
        "    'freight_value': 'freight',\n",
        "    'payment_type': 'payment_method'\n",
        "})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:50.338309Z",
          "iopub.execute_input": "2025-08-14T01:42:50.338622Z",
          "iopub.status.idle": "2025-08-14T01:42:50.354997Z",
          "shell.execute_reply.started": "2025-08-14T01:42:50.338596Z",
          "shell.execute_reply": "2025-08-14T01:42:50.35426Z"
        },
        "id": "TNJhaXMmkRwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = merged_data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:50.719141Z",
          "iopub.execute_input": "2025-08-14T01:42:50.719402Z",
          "iopub.status.idle": "2025-08-14T01:42:50.723042Z",
          "shell.execute_reply.started": "2025-08-14T01:42:50.719383Z",
          "shell.execute_reply": "2025-08-14T01:42:50.722254Z"
        },
        "id": "NToF37hKkRwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:42:51.219884Z",
          "iopub.execute_input": "2025-08-14T01:42:51.220604Z",
          "iopub.status.idle": "2025-08-14T01:42:51.235071Z",
          "shell.execute_reply.started": "2025-08-14T01:42:51.220574Z",
          "shell.execute_reply": "2025-08-14T01:42:51.234297Z"
        },
        "id": "Tr-IZyAhkRwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Retival Rag"
      ],
      "metadata": {
        "id": "C6Wummz5kRwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis Agent"
      ],
      "metadata": {
        "id": "Ai5T745qkRwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T01:45:18.120916Z",
          "iopub.execute_input": "2025-08-14T01:45:18.121846Z",
          "iopub.status.idle": "2025-08-14T01:45:18.125726Z",
          "shell.execute_reply.started": "2025-08-14T01:45:18.121822Z",
          "shell.execute_reply": "2025-08-14T01:45:18.124954Z"
        },
        "id": "lZEcvrOLkRwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Language Model Setup\n",
        "# This cell sets up the Large Language Model (LLM) and its pipeline.\n",
        "# Load the tokenizer and model for the T5 family.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "# Create a text-generation pipeline using the model and tokenizer.\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=0)\n",
        "\n",
        "# Wrap the pipeline in a LangChain HuggingFacePipeline for easy integration.\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"HuggingFace pipeline and LLM initialized successfully.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:03:24.830757Z",
          "iopub.execute_input": "2025-08-14T02:03:24.831662Z",
          "iopub.status.idle": "2025-08-14T02:03:47.218564Z",
          "shell.execute_reply.started": "2025-08-14T02:03:24.831629Z",
          "shell.execute_reply": "2025-08-14T02:03:47.217889Z"
        },
        "id": "1tspX7HHkRwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary mapping a keyword to the corresponding pandas expression.\n",
        "PANDAS_EXPRESSIONS = {\n",
        "    # General order information\n",
        "    \"total_orders\": \"len(data)\",\n",
        "    \"order_statuses_count\": \"data['order_status'].value_counts()\",\n",
        "    \"unique_product_categories_count\": \"data['category'].nunique()\",\n",
        "\n",
        "    # Financial insights\n",
        "    \"total_sales_value\": \"data['price'].sum()\",\n",
        "    \"total_freight_value\": \"data['freight'].sum()\",\n",
        "    \"total_payment_value\": \"data['payment_value'].sum()\",\n",
        "    \"avg_payment_value\": \"data['payment_value'].mean()\",\n",
        "    \"avg_freight_value\": \"data['freight'].mean()\",\n",
        "    \"avg_price_per_product\": \"data['price'].mean()\",\n",
        "    \"most_expensive_product_price\": \"data['price'].max()\",\n",
        "    \"least_expensive_product_price\": \"data['price'].min()\",\n",
        "    \"median_price\": \"data['price'].median()\", # New: Median price\n",
        "    \"std_dev_price\": \"data['price'].std()\", # New: Standard deviation of prices\n",
        "\n",
        "    # Product and category insights\n",
        "    \"count_orders_per_category\": \"data.groupby('category')['order_status'].count()\",\n",
        "    \"avg_price_per_category\": \"data.groupby('category')['price'].mean()\",\n",
        "    \"top_3_categories_by_sales\": \"data.groupby('category')['price'].sum().nlargest(3)\",\n",
        "    \"top_5_categories_by_sales\": \"data.groupby('category')['price'].sum().nlargest(5)\",\n",
        "    \"bottom_5_categories_by_sales\": \"data.groupby('category')['price'].sum().nsmallest(5)\",\n",
        "    \"most_popular_category\": \"data['category'].mode()[0]\", # New: Most popular category\n",
        "\n",
        "    # Payment and review analysis\n",
        "    \"reviews_per_category\": \"data.groupby('category')['review'].count()\",\n",
        "    \"most_common_payment_type\": \"data['payment_method'].mode()[0]\",\n",
        "    \"avg_review_score\": \"data['review'].mean()\",\n",
        "    \"reviews_by_score\": \"data['review'].value_counts().sort_index()\",\n",
        "    \"reviews_per_state\": \"data.groupby('customer_state')['review'].mean()\", # New: Average review score per state\n",
        "\n",
        "    # Geographic data\n",
        "    \"city_with_most_orders\": \"data['customer_city'].mode()[0]\",\n",
        "    \"state_with_most_orders\": \"data['customer_state'].mode()[0]\",\n",
        "    \"top_5_cities_by_orders\": \"data['customer_city'].value_counts().nlargest(5)\",\n",
        "    \"top_5_states_by_orders\": \"data['customer_state'].value_counts().nlargest(5)\",\n",
        "    \"top_3_states_by_sales\": \"data.groupby('customer_state')['price'].sum().nlargest(3)\",\n",
        "    \"sales_per_state\": \"data.groupby('customer_state')['price'].sum()\", # New: Total sales per state\n",
        "    \"orders_by_state_and_city\": \"data.groupby(['customer_state', 'customer_city'])['order_status'].count().sort_values(ascending=False)\", # New: Orders by state and city\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:03:52.164578Z",
          "iopub.execute_input": "2025-08-14T02:03:52.165214Z",
          "iopub.status.idle": "2025-08-14T02:03:52.170596Z",
          "shell.execute_reply.started": "2025-08-14T02:03:52.165192Z",
          "shell.execute_reply": "2025-08-14T02:03:52.169919Z"
        },
        "id": "kmjQzR79kRwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query_with_llm(query, df):\n",
        "    \"\"\"\n",
        "    Asks the LLM to identify a keyword, executes the pandas expression,\n",
        "    and then asks the LLM to format the result into a readable sentence.\n",
        "    \"\"\"\n",
        "    # Step 1: Ask the LLM for a keyword.\n",
        "    keyword_prompt = f\"\"\"\n",
        "    You are an expert data analyst. You are given a pandas DataFrame named 'data'.\n",
        "    Your task is to identify which of the following keywords best answers the user's query:\n",
        "\n",
        "    Keywords: {list(PANDAS_EXPRESSIONS.keys())}\n",
        "\n",
        "    Please provide only the single keyword that is the best match. Do not provide any other text or explanation.\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        keyword = llm.invoke(keyword_prompt).strip()\n",
        "\n",
        "        if keyword in PANDAS_EXPRESSIONS:\n",
        "            expression_to_run = PANDAS_EXPRESSIONS[keyword]\n",
        "            print(f\"Executing this expression: {expression_to_run}\")\n",
        "\n",
        "            # Step 2: Execute the code to get the raw result.\n",
        "            raw_result = eval(expression_to_run, {'data': df, 'pd': pd})\n",
        "\n",
        "            # Step 3: Create a new prompt to format the output.\n",
        "            formatting_prompt = f\"\"\"\n",
        "            You are an expert data analyst. The user asked a question and you have the result of a data query.\n",
        "            Please format the raw result into a clear, professional, and conversational sentence.\n",
        "            Do not just print the numbers. Explain what they mean.\n",
        "\n",
        "            User Query: {query}\n",
        "            Raw Result: {raw_result}\n",
        "\n",
        "            Formatted Answer:\n",
        "            \"\"\"\n",
        "\n",
        "            # Step 4: Ask the LLM to format the result.\n",
        "            formatted_answer = llm.invoke(formatting_prompt).strip()\n",
        "\n",
        "            return formatted_answer\n",
        "\n",
        "        else:\n",
        "            return f\"The LLM returned an invalid keyword: {keyword}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while executing the code: {e}\"\n",
        "\n",
        "print(\"Custom query function defined with a new, more robust approach.\")\n",
        "\n",
        "# Cell 4: Interactive Query with a continuous loop\n",
        "# This cell takes user input in a loop and prints the final answer.\n",
        "while True:\n",
        "    user_query = input(\"Give me Your Question About Your Data (type 'exit' to quit): \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        break\n",
        "    response = run_query_with_llm(user_query, data)\n",
        "    print(f\"Insight: {response}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:04:00.732479Z",
          "iopub.execute_input": "2025-08-14T02:04:00.732756Z",
          "execution_failed": "2025-08-14T03:03:10.347Z"
        },
        "id": "QZ_wbhBFkRwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recomender Agent"
      ],
      "metadata": {
        "id": "NtuRZ2BGkRwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query_with_llm(query, df):\n",
        "    \"\"\"\n",
        "    Asks the LLM to identify a keyword, executes the pandas expression,\n",
        "    and then asks the LLM to format the result into a readable sentence.\n",
        "    Returns both the formatted answer and the keyword.\n",
        "    \"\"\"\n",
        "    keyword_prompt = f\"\"\"\n",
        "    You are an expert data analyst. You are given a pandas DataFrame named 'data'.\n",
        "    Your task is to identify which of the following keywords best answers the user's query:\n",
        "\n",
        "    Keywords: {list(PANDAS_EXPRESSIONS.keys())}\n",
        "\n",
        "    Please provide only the single keyword that is the best match. Do not provide any other text or explanation.\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Response:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keyword = llm.invoke(keyword_prompt).strip()\n",
        "        if keyword in PANDAS_EXPRESSIONS:\n",
        "            expression_to_run = PANDAS_EXPRESSIONS[keyword]\n",
        "            print(f\"Executing this expression: {expression_to_run}\")\n",
        "            raw_result = eval(expression_to_run, {'data': df, 'pd': pd})\n",
        "            formatting_prompt = f\"\"\"\n",
        "            You are an expert data analyst. The user asked a question and you have the result of a data query.\n",
        "            Please format the raw result into a clear, professional, and conversational sentence.\n",
        "            Do not just print the numbers. Explain what they mean.\n",
        "            User Query: {query}\n",
        "            Raw Result: {raw_result}\n",
        "            Formatted Answer:\n",
        "            \"\"\"\n",
        "            formatted_answer = llm.invoke(formatting_prompt).strip()\n",
        "            return formatted_answer, keyword\n",
        "        else:\n",
        "            return f\"The LLM returned an invalid keyword: {keyword}\", None\n",
        "    except Exception as e:\n",
        "        # Fixed: This line now returns two values to prevent the ValueError\n",
        "        return f\"An error occurred while executing the code: {e}\", None\n",
        "\n",
        "print(\"Custom query function defined with a new, more robust approach.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:02:31.476307Z",
          "iopub.execute_input": "2025-08-14T02:02:31.477046Z",
          "iopub.status.idle": "2025-08-14T02:02:31.483735Z",
          "shell.execute_reply.started": "2025-08-14T02:02:31.47702Z",
          "shell.execute_reply": "2025-08-14T02:02:31.482848Z"
        },
        "id": "3NhD6obekRwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Interactive Query with a continuous loop for Insights and Recommendations\n",
        "# This cell takes user input, generates an insight, and then generates an e-commerce recommendation.\n",
        "\n",
        "# Create a second LLM instance, as requested.\n",
        "llm2 = llm\n",
        "\n",
        "# Define the prompt for generating e-commerce recommendations.\n",
        "RECOMMENDATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert e-commerce marketing consultant. Your task is to provide a detailed, actionable, and comprehensive recommendation to a business owner based on a data-driven insight.\n",
        "\n",
        "Please use the following format for your response:\n",
        "### Key Insight Summary\n",
        "Briefly summarize the insight you've been given.\n",
        "\n",
        "### Importance for the Business\n",
        "Explain why this insight is relevant and important for an e-commerce business. What opportunities or challenges does it present?\n",
        "\n",
        "### Actionable Recommendations\n",
        "Provide a list of 2-3 specific and practical steps the business owner can take to leverage this insight. Be creative and think about marketing, inventory, or customer strategy.\n",
        "\n",
        "### Expected Impact\n",
        "Conclude with a sentence about the potential positive impact of these actions.\n",
        "\n",
        "Insight from data analyst: {insight}\n",
        "\n",
        "Detailed Recommendation:\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"Give me Your Question About Your Data (type 'exit' to quit): \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Step 1: Get the insightful answer from the data analyst.\n",
        "    insightful_answer = run_query_with_llm(user_query, data)\n",
        "    print(f\"Insight: {insightful_answer}\")\n",
        "\n",
        "    # Step 2: Use the second LLM to generate a detailed recommendation.\n",
        "    recommendation_prompt = RECOMMENDATION_PROMPT_TEMPLATE.format(insight=insightful_answer)\n",
        "    recommendation = llm2.invoke(recommendation_prompt).strip()\n",
        "\n",
        "    print(f\"Recommendation: {recommendation}\\n\" + \"-\"*50)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:02:37.311212Z",
          "iopub.execute_input": "2025-08-14T02:02:37.311532Z",
          "iopub.status.idle": "2025-08-14T02:03:19.366023Z",
          "shell.execute_reply.started": "2025-08-14T02:02:37.311507Z",
          "shell.execute_reply": "2025-08-14T02:03:19.365446Z"
        },
        "id": "VRcGSWOEkRwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average total price of an order?\n",
        "\n",
        "What is the total value of all payments?\n",
        "\n",
        "Which payment method is the most common?\n",
        "\n",
        "What are the top 5 categories by total sales value?\n",
        "\n",
        "What is the average price for each product category?\n",
        "\n",
        "How many unique product categories are there?"
      ],
      "metadata": {
        "id": "UIcRq0fvkRwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit App For Main Feature"
      ],
      "metadata": {
        "id": "--5CiIXskRwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install necessary libraries\n",
        "# This includes all the libraries needed for the app to run.\n",
        "!pip install streamlit pyngrok transformers accelerate langchain_community --quiet\n",
        "!pip install --upgrade \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:17:30.453312Z",
          "iopub.execute_input": "2025-08-14T02:17:30.453816Z",
          "iopub.status.idle": "2025-08-14T02:19:03.073522Z",
          "shell.execute_reply.started": "2025-08-14T02:17:30.45377Z",
          "shell.execute_reply": "2025-08-14T02:19:03.072851Z"
        },
        "id": "RClLHWS7kRwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Write the Streamlit app file ---\n",
        "# The app logic is saved to a file named 'app.py'\n",
        "app_py_content = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# IMPORTANT: The provided API keys are not strictly needed for this\n",
        "# application, as we are using a free, local model (flan-t5-xl) and not a reranker.\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "# --- LLM and Agent Logic ---\n",
        "@st.cache_resource\n",
        "def get_llm_and_agent_components():\n",
        "    \"\"\"Loads LLM and defines the agent's expressions and prompts.\"\"\"\n",
        "    # Load the specified larger model: flan-t5-xl\n",
        "    # Switching from 'google/flan-t5-xl' to 'google/flan-t5-base' to save memory.\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "    llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "    # Check for GPU and set device accordingly\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    llm_pipe = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_length=512, device=device)\n",
        "    llm = HuggingFacePipeline(pipeline=llm_pipe)\n",
        "\n",
        "    # --- IMPORTANT: These expressions are specific to your dataset columns. ---\n",
        "    # The agent's logic relies on these column names. If you upload a different CSV,\n",
        "    # you may need to adjust these expressions to match your new column names.\n",
        "    PANDAS_EXPRESSIONS = {\n",
        "        \"total_orders\": \"len(data)\",\n",
        "        \"order_statuses_count\": \"data['order_status'].value_counts()\",\n",
        "        \"unique_product_categories_count\": \"data['category'].nunique()\",\n",
        "        \"total_sales_value\": \"data['price'].sum()\",\n",
        "        \"avg_price_per_product\": \"data['price'].mean()\",\n",
        "        \"count_orders_per_category\": \"data.groupby('category')['order_status'].count()\",\n",
        "        \"avg_price_per_category\": \"data.groupby('category')['price'].mean()\",\n",
        "        \"top_5_categories_by_sales\": \"data.groupby('category')['price'].sum().nlargest(5)\",\n",
        "        \"most_common_payment_type\": \"data['payment_method'].mode()[0]\",\n",
        "        \"avg_review_score\": \"data['review'].mean()\",\n",
        "        \"top_5_states_by_orders\": \"data['customer_state'].value_counts().nlargest(5)\",\n",
        "        \"sales_per_state\": \"data.groupby('customer_state')['price'].sum()\",\n",
        "    }\n",
        "\n",
        "    RECOMMENDATION_PROMPT_TEMPLATE = \"\"\"\n",
        "    You are an expert e-commerce marketing consultant. Your task is to provide a detailed, actionable, and comprehensive recommendation to a business owner based on a data-driven insight.\n",
        "\n",
        "    Insight from data analyst: {insight}\n",
        "\n",
        "    Detailed Recommendation:\n",
        "    \"\"\"\n",
        "\n",
        "    INSIGHT_FORMATTING_PROMPT = \"\"\"\n",
        "    You are an expert data analyst. The user asked a question and you have the result of a data query.\n",
        "    Please format the raw result into a clear, professional, and conversational sentence.\n",
        "    Do not just print the numbers. Explain what they mean.\n",
        "    User Query: {query}\n",
        "    Raw Result: {raw_result}\n",
        "    Formatted Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    return llm, PANDAS_EXPRESSIONS, RECOMMENDATION_PROMPT_TEMPLATE, INSIGHT_FORMATTING_PROMPT\n",
        "\n",
        "def run_query_with_llm(query, df, llm, expressions, insight_prompt, recommendation_prompt_template):\n",
        "    \"\"\"\n",
        "    Identifies the best pandas expression, executes it, and generates insight and recommendation.\n",
        "    \"\"\"\n",
        "    keyword_prompt = f\"\"\"\n",
        "    You are an expert data analyst. You are given a pandas DataFrame named 'data'.\n",
        "    Your task is to identify which of the following keywords best answers the user's query:\n",
        "    Keywords: {list(expressions.keys())}\n",
        "    Please provide only the single keyword that is the best match. Do not provide any other text or explanation.\n",
        "    Query: {query}\n",
        "    Response:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keyword = llm.invoke(keyword_prompt).strip()\n",
        "        if keyword in expressions:\n",
        "            expression_to_run = expressions[keyword]\n",
        "            raw_result = eval(expression_to_run, {'data': df, 'pd': pd})\n",
        "\n",
        "            insight_prompt_filled = insight_prompt.format(query=query, raw_result=raw_result)\n",
        "            insight = llm.invoke(insight_prompt_filled).strip()\n",
        "\n",
        "            recommendation_prompt_filled = recommendation_prompt_template.format(insight=insight)\n",
        "            recommendation = llm.invoke(recommendation_prompt_filled).strip()\n",
        "\n",
        "            return insight, recommendation\n",
        "        else:\n",
        "            return \"Sorry, I can't find an appropriate analysis for this question.\", \"\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\", \"\"\n",
        "\n",
        "# --- Streamlit App UI ---\n",
        "st.set_page_config(page_title=\"E-commerce Analytics Chatbot\", page_icon=\"🛍️\", layout=\"wide\")\n",
        "st.title(\"🛍️ E-commerce Analytics Chatbot\")\n",
        "\n",
        "# --- Initialize Session State ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"data\" not in st.session_state:\n",
        "    st.session_state.data = None\n",
        "\n",
        "# --- File Upload Section ---\n",
        "st.header(\"Upload Your CSV File\")\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the uploaded file into a DataFrame\n",
        "    try:\n",
        "        st.session_state.data = pd.read_csv(uploaded_file)\n",
        "        st.success(\"File uploaded successfully! You can now start the chat below.\")\n",
        "        st.dataframe(st.session_state.data.head())\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "        st.session_state.data = None\n",
        "\n",
        "# --- Chat Interface ---\n",
        "if st.session_state.data is not None:\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Start Chatting\")\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"What would you like to know about your data?\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Analyzing data...\"):\n",
        "                llm, PANDAS_EXPRESSIONS, RECOMMENDATION_PROMPT_TEMPLATE, INSIGHT_FORMATTING_PROMPT = get_llm_and_agent_components()\n",
        "                insight, recommendation = run_query_with_llm(\n",
        "                    prompt,\n",
        "                    st.session_state.data,\n",
        "                    llm,\n",
        "                    PANDAS_EXPRESSIONS,\n",
        "                    INSIGHT_FORMATTING_PROMPT,\n",
        "                    RECOMMENDATION_PROMPT_TEMPLATE\n",
        "                )\n",
        "\n",
        "                # --- This multi-line f-string is now correctly formatted ---\n",
        "                full_response = f\"\"\"**Insight:**\n",
        "{insight}\n",
        "\n",
        "**Recommendation:**\n",
        "{recommendation}\"\"\"\n",
        "                st.markdown(full_response)\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "else:\n",
        "    st.info(\"Please upload a CSV file to begin.\")\n",
        "'''\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_py_content)\n",
        "\n",
        "# --- 3. Run the Streamlit app with ngrok ---\n",
        "# This part runs the Streamlit app and opens a public URL for it using ngrok\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "from pyngrok import ngrok, conf\n",
        "import os\n",
        "\n",
        "# Set your ngrok auth token.\n",
        "NGROK_AUTH_TOKEN = \"31CpEH0DnLmMSw7dD2KeI7wYvq4_3bf1GVWy4pAPVTqRvGoB9\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "def run_streamlit_app():\n",
        "    # Start streamlit app in background\n",
        "    # Note: Streamlit may take a bit longer to start with a larger model (flan-t5-xl)\n",
        "    proc = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "    # Wait for the Streamlit app to start\n",
        "    print(\"Waiting for Streamlit app to start...\")\n",
        "    # Add a counter for timeout\n",
        "    timeout = 180 # Increased timeout to 3 minutes for a very large model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Check if the server is up and running before connecting ngrok\n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            # Try to connect to the Streamlit port\n",
        "            response = requests.get(\"http://localhost:8501\")\n",
        "            if response.status_code == 200:\n",
        "                print(\"Streamlit app is running!\")\n",
        "                break\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\"Streamlit not ready yet. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        # If the loop completes without breaking, the app failed to start\n",
        "        print(\"Error: Streamlit app failed to start within the timeout period.\")\n",
        "        proc.kill()\n",
        "        return\n",
        "\n",
        "    # Open ngrok tunnel\n",
        "    try:\n",
        "        # Kill all existing ngrok tunnels before creating a new one\n",
        "        ngrok.kill()\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "        print(f\"Your Streamlit app is running at: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start ngrok tunnel: {e}\")\n",
        "        ngrok.kill()\n",
        "        proc.kill()\n",
        "        raise\n",
        "\n",
        "# Call the function to run the app\n",
        "run_streamlit_app()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T02:19:06.475561Z",
          "iopub.execute_input": "2025-08-14T02:19:06.475856Z",
          "iopub.status.idle": "2025-08-14T02:19:13.649494Z",
          "shell.execute_reply.started": "2025-08-14T02:19:06.475831Z",
          "shell.execute_reply": "2025-08-14T02:19:13.648939Z"
        },
        "id": "2rVTjnhbkRwS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ecommerce ChatBOT Feature"
      ],
      "metadata": {
        "id": "J4mRw9G0kRwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for RAG\n",
        "!pip install -qU transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T08:52:35.252609Z",
          "iopub.execute_input": "2025-08-14T08:52:35.252924Z",
          "iopub.status.idle": "2025-08-14T08:54:05.213338Z",
          "shell.execute_reply.started": "2025-08-14T08:52:35.252899Z",
          "shell.execute_reply": "2025-08-14T08:54:05.212412Z"
        },
        "id": "82Ci875HkRwT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T08:55:58.62827Z",
          "iopub.execute_input": "2025-08-14T08:55:58.62861Z",
          "iopub.status.idle": "2025-08-14T08:56:06.693273Z",
          "shell.execute_reply.started": "2025-08-14T08:55:58.628578Z",
          "shell.execute_reply": "2025-08-14T08:56:06.692744Z"
        },
        "id": "Yw2bK6n_kRwT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Hugging Face token directly. This is NOT recommended for production.\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "\n",
        "# Define a function to load the model and tokenizer\n",
        "def load_llm_model(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", hf_token=None):\n",
        "    \"\"\"\n",
        "    Loads a Hugging Face LLM model and tokenizer using 4-bit quantization.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): The name of the model to load from Hugging Face.\n",
        "    - hf_token (str, optional): The Hugging Face Access Token.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing the loaded tokenizer and model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model: {model_name}...\")\n",
        "\n",
        "    # Configure 4-bit quantization for efficient memory usage\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=False\n",
        "    )\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "\n",
        "    # Load the model with quantization and move it to the GPU\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=quantization_config,\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    print(f\"Model {model_name} loaded successfully!\")\n",
        "    return tokenizer, model\n",
        "\n",
        "# Execute the function to load the model using the hardcoded token\n",
        "if hf_token:\n",
        "    llm_tokenizer, llm_model = load_llm_model(hf_token=hf_token)\n",
        "else:\n",
        "    print(\"Hugging Face token not found.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T08:56:09.405904Z",
          "iopub.execute_input": "2025-08-14T08:56:09.406762Z",
          "iopub.status.idle": "2025-08-14T08:59:19.870445Z",
          "shell.execute_reply.started": "2025-08-14T08:56:09.406736Z",
          "shell.execute_reply": "2025-08-14T08:59:19.869809Z"
        },
        "id": "BqltBjJukRwT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Books Uploading & Embedding"
      ],
      "metadata": {
        "id": "fzQoEuu8kRwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf faiss-cpu sentence-transformers cohere langchain-cohere langchain-core -q\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:32:59.931786Z",
          "iopub.execute_input": "2025-08-14T10:32:59.932074Z",
          "iopub.status.idle": "2025-08-14T10:34:21.65291Z",
          "shell.execute_reply.started": "2025-08-14T10:32:59.932053Z",
          "shell.execute_reply": "2025-08-14T10:34:21.652096Z"
        },
        "id": "Q6lLt-fEkRwU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:34:21.654797Z",
          "iopub.execute_input": "2025-08-14T10:34:21.655082Z",
          "iopub.status.idle": "2025-08-14T10:34:23.294486Z",
          "shell.execute_reply.started": "2025-08-14T10:34:21.655058Z",
          "shell.execute_reply": "2025-08-14T10:34:23.293899Z"
        },
        "id": "orKKQqGnkRwU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from a specified directory\n",
        "def load_documents(directory):\n",
        "    \"\"\"\n",
        "    Loads all PDF documents from a given directory.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            documents.extend(loader.load())\n",
        "            print(f\"Loaded {file_name}\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "def split_documents(documents: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits a list of documents into smaller, more manageable chunks.\n",
        "    \"\"\"\n",
        "    # We will use RecursiveCharacterTextSplitter for more effective chunking\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    text_chunks = text_splitter.split_documents(documents)\n",
        "    return text_chunks\n",
        "\n",
        "\n",
        "# Function to create and save the vector store\n",
        "def create_vector_store(chunks, save_path=\"faiss_index\"):\n",
        "    \"\"\"\n",
        "    Creates and saves a FAISS vector store from text chunks.\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "    vector_store.save_local(save_path)\n",
        "    print(f\"Vector store saved to {save_path}\")\n",
        "    return vector_store"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:34:23.295187Z",
          "iopub.execute_input": "2025-08-14T10:34:23.295558Z",
          "iopub.status.idle": "2025-08-14T10:34:23.301623Z",
          "shell.execute_reply.started": "2025-08-14T10:34:23.295538Z",
          "shell.execute_reply": "2025-08-14T10:34:23.300806Z"
        },
        "id": "W1GmpLV2kRwa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the functions to create and save the vector store\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the directory where your dataset is located\n",
        "    data_directory = \"/kaggle/input/e-commercee-books\"\n",
        "\n",
        "    # 1. Load documents\n",
        "    loaded_documents = load_documents(data_directory)\n",
        "\n",
        "    if loaded_documents:\n",
        "        # 2. Split documents into chunks\n",
        "        text_chunks = split_documents(loaded_documents)\n",
        "        print(f\"Total chunks created: {len(text_chunks)}\")\n",
        "\n",
        "        # 3. Create and save the vector store\n",
        "        vector_store = create_vector_store(text_chunks)\n",
        "    else:\n",
        "        print(\"No PDF documents found. Please ensure your files are in the specified dataset.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:34:23.303289Z",
          "iopub.execute_input": "2025-08-14T10:34:23.303824Z",
          "iopub.status.idle": "2025-08-14T10:35:12.539814Z",
          "shell.execute_reply.started": "2025-08-14T10:34:23.303799Z",
          "shell.execute_reply": "2025-08-14T10:35:12.539127Z"
        },
        "id": "ncqKQWoGkRwa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History-Aware & Re-Ranking RAG"
      ],
      "metadata": {
        "id": "ZsSjISJlkRwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Pipline"
      ],
      "metadata": {
        "id": "_3-7W-HCkRwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import warnings\n",
        "import transformers\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from collections import deque\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:35:12.540517Z",
          "iopub.execute_input": "2025-08-14T10:35:12.540717Z",
          "iopub.status.idle": "2025-08-14T10:35:13.951282Z",
          "shell.execute_reply.started": "2025-08-14T10:35:12.540694Z",
          "shell.execute_reply": "2025-08-14T10:35:13.950641Z"
        },
        "id": "-VTT_W7KkRwb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load FAISS Index ---\n",
        "def load_faiss_index(index_path=\"faiss_index\"):\n",
        "    \"\"\"Loads a previously saved FAISS vector store.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    faiss_index = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "    return faiss_index\n",
        "\n",
        "# --- 2. Create Re-ranker ---\n",
        "def create_reranker(cohere_api_key):\n",
        "    \"\"\"Initializes a Cohere Re-ranker.\"\"\"\n",
        "    return CohereRerank(cohere_api_key=cohere_api_key, model=\"rerank-english-v3.0\", top_n=3)\n",
        "\n",
        "# Load the FAISS index\n",
        "faiss_index = load_faiss_index()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:35:13.951958Z",
          "iopub.execute_input": "2025-08-14T10:35:13.95256Z",
          "iopub.status.idle": "2025-08-14T10:35:15.162761Z",
          "shell.execute_reply.started": "2025-08-14T10:35:13.952539Z",
          "shell.execute_reply": "2025-08-14T10:35:15.162168Z"
        },
        "id": "vLbhWNYtkRwb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# --- Load Cohere API Key from Kaggle Secrets ---\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    cohere_api_key = user_secrets.get_secret(\"COHERE_API_KEY\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading Cohere API key from Kaggle secrets: {e}\")\n",
        "\n",
        "# Create the reranker instance\n",
        "reranker = create_reranker(cohere_api_key)\n",
        "\n",
        "# --- 2. Create the LLM Pipeline ---\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=llm_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# --- 3. Initialize chat loop variables ---\n",
        "print(\"Chatbot is ready. You can start asking questions. Type 'exit' to end the chat.\")\n",
        "\n",
        "chat_history = \"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:59:47.261659Z",
          "iopub.execute_input": "2025-08-14T09:59:47.262299Z",
          "iopub.status.idle": "2025-08-14T09:59:47.579395Z",
          "shell.execute_reply.started": "2025-08-14T09:59:47.262272Z",
          "shell.execute_reply": "2025-08-14T09:59:47.578624Z"
        },
        "id": "5SSS4bkckRwb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final ChatBot Test"
      ],
      "metadata": {
        "id": "P0MfzR6dkRwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What are the benefits of E-Marketing?\n",
        "- Can you elaborate on 'E-Marketing' as a concept?"
      ],
      "metadata": {
        "id": "J6HeaGsXkRwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A double-ended queue to store the history.\n",
        "chat_history_list = deque(maxlen=5)\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"You: \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # Step A: Retrieve and re-rank relevant documents\n",
        "        # The base retriever will fetch the top 10 documents based on similarity.\n",
        "        base_retriever = faiss_index.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "        # The ContextualCompressionRetriever uses the reranker to select the best 3 documents from the initial 10.\n",
        "        compressed_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=reranker,\n",
        "            base_retriever=base_retriever\n",
        "        )\n",
        "\n",
        "        # Get the final re-ranked documents\n",
        "        docs = compressed_retriever.get_relevant_documents(user_query)\n",
        "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # Format the chat history list into a string for the prompt\n",
        "        chat_history = \"\"\n",
        "        for human_message, assistant_message in chat_history_list:\n",
        "            chat_history += f\"Human: {human_message}\\nAssistant: {assistant_message}\\n\"\n",
        "\n",
        "        # Step B: Build the improved prompt\n",
        "        prompt_template = f\"\"\"\n",
        "        You are an e-commerce assistant. Your goal is to provide concise and helpful answers based only on the provided context and chat history.\n",
        "        Do not make up information. If the answer is not in the provided documents, say \"I don't know the answer based on the provided context.\"\n",
        "\n",
        "        Chat History:\n",
        "        {chat_history}\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {user_query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "        # Step C: Generate the response from the LLM pipeline\n",
        "        response = pipeline(prompt_template)[0]['generated_text']\n",
        "\n",
        "        # Step D: Post-process the response to get only the answer\n",
        "        if \"Answer:\" in response:\n",
        "            answer_text = response.split(\"Answer:\", 1)[1].strip()\n",
        "        else:\n",
        "            answer_text = response.strip()\n",
        "\n",
        "        print(f\"Chatbot: {answer_text}\")\n",
        "\n",
        "        # Step E: Update the history list for the next turn\n",
        "        chat_history_list.append((user_query, answer_text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4iQc5Vo_kRwc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit App For ChatBot"
      ],
      "metadata": {
        "id": "bk2OZ0p_kRwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for the Streamlit app and ngrok\n",
        "# Run this code in a single cell inside a Kaggle Notebook\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "# Install libraries for the RAG app\n",
        "!pip install -q -U \"langchain[full]\" cohere huggingface_hub transformers bitsandbytes accelerate sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:07:23.329207Z",
          "iopub.status.idle": "2025-08-14T10:07:23.32948Z",
          "shell.execute_reply.started": "2025-08-14T10:07:23.329341Z",
          "shell.execute_reply": "2025-08-14T10:07:23.329351Z"
        },
        "id": "QYrsMudckRwc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "import subprocess\n",
        "from threading import Thread\n",
        "import time\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.docstore.document import Document\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from pyngrok import ngrok, conf\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# --- 1. API Keys (Must be replaced) ---\n",
        "# WARNING: In a production environment, it is recommended to use Kaggle Secrets.\n",
        "# IMPORTANT: Replace the following values with your actual API keys.\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "# --- 2. ngrok Setup ---\n",
        "# The ngrok auth token is hardcoded here as requested.\n",
        "# Please remember that for security best practices, it is recommended to use\n",
        "# Kaggle Secrets.\n",
        "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# --- 3. Write the Streamlit app file ---\n",
        "# This part of the code creates an app.py file containing the complete RAG application code.\n",
        "app_py_content = \"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# IMPORTANT: Replace these values with your actual API keys.\n",
        "HUGGINGFACE_TOKEN = \"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\"\n",
        "COHERE_API_KEY = \"BFRQTKUXpMbErJChHDGaiOyMQ6RjZ9VtRjCA580G\"\n",
        "\n",
        "@st.cache_resource\n",
        "def get_rag_chain():\n",
        "    \\\"\\\"\\\"\n",
        "    Loads all components and assembles the RAG chain.\n",
        "    This function runs only once due to @st.cache_resource.\n",
        "    \\\"\\\"\\\"\n",
        "    # Configure 4-bit quantization for efficient memory usage\n",
        "    if torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=False\n",
        "        )\n",
        "        device_map = \"auto\"\n",
        "    else:\n",
        "        st.warning(\"CUDA is not available. Running the model on CPU may be very slow.\")\n",
        "        quantization_config = None\n",
        "        device_map = None\n",
        "\n",
        "    # Load the LLM Model and tokenizer\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=device_map,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=quantization_config,\n",
        "        token=HUGGINGFACE_TOKEN\n",
        "    )\n",
        "    llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
        "\n",
        "    # Create the LLM pipeline\n",
        "    pipeline = HuggingFacePipeline(\n",
        "        pipeline=transformers.pipeline(\n",
        "            \"text-generation\",\n",
        "            model=llm_model,\n",
        "            tokenizer=llm_tokenizer,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Load the FAISS Index\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    faiss_index_path = \"faiss_index\"\n",
        "\n",
        "    try:\n",
        "        faiss_index = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading FAISS index. Please ensure the 'faiss_index' directory exists and is correctly populated: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Create the Reranker\n",
        "    reranker = CohereRerank(cohere_api_key=COHERE_API_KEY, model=\"rerank-english-v3.0\", top_n=5)\n",
        "\n",
        "    # Create the Retriever with Re-ranking\n",
        "    base_retriever = faiss_index.as_retriever(search_kwargs={\"k\": 20})\n",
        "    compressed_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=reranker,\n",
        "        base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    # Create the Memory and Prompt\n",
        "    prompt_template = \\\"\\\"\\\"You are an e-commerce assistant. Use the following context and chat history to answer the question.\n",
        "    If the answer is not in the provided documents, say \"I don't know the answer based on the provided context.\"\n",
        "    Chat History:\n",
        "    {chat_history}\n",
        "    Context:\n",
        "    {context}\n",
        "    Question:\n",
        "    {question}\n",
        "    Answer:\\\"\\\"\\\"\n",
        "\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        k=3,\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True\n",
        "    )\n",
        "\n",
        "    # Assemble the full Conversational RAG Chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=pipeline,\n",
        "        retriever=compressed_retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\n",
        "            \"prompt\": PromptTemplate(template=prompt_template, input_variables=[\"chat_history\", \"context\", \"question\"])\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# --- Streamlit App UI ---\n",
        "rag_chain = get_rag_chain()\n",
        "st.set_page_config(page_title=\"E-Commerce Assistant\", page_icon=\"🛍️\")\n",
        "st.title(\"E-Commerce Assistant\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"How can I help you?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            # Invoke the RAG chain\n",
        "            response = rag_chain.invoke({\"question\": prompt})\n",
        "            answer_text = response.get('answer', '')\n",
        "\n",
        "            # --- Robust answer cleaning logic to only return the answer ---\n",
        "            # This logic mimics the successful interactive code to ensure only the final answer is shown.\n",
        "            cleaned_answer = \"\"\n",
        "            # The model's output might contain the whole prompt, we need to extract the part after 'Answer:'\n",
        "            if \"Answer:\" in answer_text:\n",
        "                # Find the last occurrence of \"Answer:\" to handle cases where it might appear in chat history\n",
        "                last_answer_index = answer_text.rfind(\"Answer:\")\n",
        "                cleaned_answer = answer_text[last_answer_index + len(\"Answer:\"):].strip()\n",
        "            else:\n",
        "                # As a fallback, simply take the entire text if the label is missing\n",
        "                cleaned_answer = answer_text.strip()\n",
        "\n",
        "            st.markdown(cleaned_answer)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": cleaned_answer})\n",
        "\"\"\"\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_py_content)\n",
        "\n",
        "# --- 4. Run the Streamlit app with ngrok ---\n",
        "# This part runs the Streamlit app and opens a public URL for it using ngrok\n",
        "def run_streamlit_app():\n",
        "    # Install pyngrok\n",
        "    os.system(\"pip install pyngrok --quiet\")\n",
        "\n",
        "    # Start streamlit app in background\n",
        "    proc = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "    # Wait for the Streamlit app to start on port 8501\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Open ngrok tunnel to the Streamlit app\n",
        "    try:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "        print(f\"Your Streamlit app is running at: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start ngrok tunnel: {e}\")\n",
        "        ngrok.kill()\n",
        "        proc.kill()\n",
        "        raise\n",
        "\n",
        "run_streamlit_app()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T09:16:05.499045Z",
          "iopub.execute_input": "2025-08-14T09:16:05.499854Z",
          "iopub.status.idle": "2025-08-14T09:16:27.968855Z",
          "shell.execute_reply.started": "2025-08-14T09:16:05.499822Z",
          "shell.execute_reply": "2025-08-14T09:16:27.968242Z"
        },
        "id": "eOITVhaqkRwc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Streamlit App"
      ],
      "metadata": {
        "id": "PbjjoqPikRwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok transformers accelerate langchain_community langchain-cohere huggingface_hub bitsandbytes sentence-transformers faiss-cpu --quiet"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:35:15.163493Z",
          "iopub.execute_input": "2025-08-14T10:35:15.163713Z",
          "iopub.status.idle": "2025-08-14T10:35:22.598805Z",
          "shell.execute_reply.started": "2025-08-14T10:35:15.163694Z",
          "shell.execute_reply": "2025-08-14T10:35:22.59782Z"
        },
        "id": "kki82IEukRwd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "from pyngrok import ngrok, conf\n",
        "import shutil\n",
        "\n",
        "# Set your ngrok auth token.\n",
        "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# --- 1. Define the content for each app file ---\n",
        "\n",
        "# Main app file (Home page)\n",
        "main_app_content = \"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"E-commerce Solutions\", page_icon=\"🛍️\", layout=\"wide\")\n",
        "st.title(\"🛍️ Integrated E-commerce Solutions Platform\")\n",
        "st.write(\\\"\\\"\\\"\n",
        "Welcome to the Integrated E-commerce Solutions Platform. This platform is specifically designed to help e-commerce store owners transform their data into valuable, actionable insights. By leveraging advanced AI tools, we provide you with the ability to understand customer behavior, analyze product performance, and make data-driven strategic and marketing decisions.\n",
        "\n",
        "Our platform consists of two main applications:\n",
        "\n",
        "1.  **Analytics Chatbot:**\n",
        "    * Allows you to upload your sales data via CSV files.\n",
        "    * Answers your questions in natural language about your store's performance.\n",
        "    * Provides instant analytics on total sales, best-selling products, customer behavior, and more.\n",
        "    * Helps you identify strengths and weaknesses in your business.\n",
        "\n",
        "2.  **RAG Assistant:**\n",
        "    * Acts as a knowledge base to help you get accurate and detailed information.\n",
        "    * Uses Retrieval Augmented Generation (RAG) technology to search your documents and provide quick, relevant answers.\n",
        "    * Ideal for getting answers about company policies, product details, or any other information you need.\n",
        "\n",
        "We believe that understanding data is the key to success in the world of e-commerce. Use the sidebar to navigate between the applications and explore the analytical power of our platform.\n",
        "\\\"\\\"\\\")\n",
        "\"\"\"\n",
        "\n",
        "# First page: Analytics Chatbot App\n",
        "analytics_app_content = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Define the model to use. Now using the larger 't5-xl' model.\n",
        "MODEL_NAME = \"google/flan-t5-xl\"\n",
        "\n",
        "# --- Centralized LLM and Agent Logic ---\n",
        "@st.cache_resource\n",
        "def get_llm_and_agent_components():\n",
        "    \\\"\\\"\\\"\n",
        "    Loads LLM and defines the agent's expressions and prompts.\n",
        "    This function is cached to ensure the model is loaded only once.\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # Load the XL model with quantization for memory efficiency.\n",
        "        if torch.cuda.is_available():\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=False\n",
        "            )\n",
        "            device_map = \"auto\"\n",
        "            llm_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                MODEL_NAME,\n",
        "                device_map=device_map,\n",
        "                quantization_config=quantization_config,\n",
        "            )\n",
        "            llm_pipe = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_length=512)\n",
        "        else:\n",
        "            st.warning(\"CUDA is not available. Running the model on CPU may be very slow.\")\n",
        "            llm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "            llm_pipe = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_length=512, device=-1)\n",
        "\n",
        "        llm = HuggingFacePipeline(pipeline=llm_pipe)\n",
        "\n",
        "        # --- IMPORTANT: These expressions are specific to your dataset columns. ---\n",
        "        # A dictionary mapping a keyword to the corresponding pandas expression.\n",
        "        PANDAS_EXPRESSIONS = {\n",
        "            # General order information\n",
        "            \"total_orders\": \"len(data)\",\n",
        "            \"order_statuses_count\": \"data['order_status'].value_counts()\",\n",
        "            \"unique_product_categories_count\": \"data['category'].nunique()\",\n",
        "\n",
        "            # Financial insights\n",
        "            \"total_sales_value\": \"data['price'].sum()\",\n",
        "            \"total_freight_value\": \"data['freight'].sum()\",\n",
        "            \"total_payment_value\": \"data['payment_value'].sum()\",\n",
        "            \"avg_payment_value\": \"data['payment_value'].mean()\",\n",
        "            \"avg_freight_value\": \"data['freight'].mean()\",\n",
        "            \"avg_price_per_product\": \"data['price'].mean()\",\n",
        "            \"most_expensive_product_price\": \"data['price'].max()\",\n",
        "            \"least_expensive_product_price\": \"data['price'].min()\",\n",
        "            \"median_price\": \"data['price'].median()\",\n",
        "            \"std_dev_price\": \"data['price'].std()\",\n",
        "\n",
        "            # Product and category insights\n",
        "            \"count_orders_per_category\": \"data.groupby('category')['order_status'].count()\",\n",
        "            \"avg_price_per_category\": \"data.groupby('category')['price'].mean()\",\n",
        "            \"top_3_categories_by_sales\": \"data.groupby('category')['price'].sum().nlargest(3)\",\n",
        "            \"top_5_categories_by_sales\": \"data.groupby('category')['price'].sum().nlargest(5)\",\n",
        "            \"bottom_5_categories_by_sales\": \"data.groupby('category')['price'].sum().nsmallest(5)\",\n",
        "            \"most_popular_category\": \"data['category'].mode()[0]\",\n",
        "\n",
        "            # Payment and review analysis\n",
        "            \"reviews_per_category\": \"data.groupby('category')['review'].count()\",\n",
        "            \"most_common_payment_type\": \"data['payment_method'].mode()[0]\",\n",
        "            \"avg_review_score\": \"data['review'].mean()\",\n",
        "            \"reviews_by_score\": \"data['review'].value_counts().sort_index()\",\n",
        "            \"reviews_per_state\": \"data.groupby('customer_state')['review'].mean()\",\n",
        "\n",
        "            # Geographic data\n",
        "            \"city_with_most_orders\": \"data['customer_city'].mode()[0]\",\n",
        "            \"state_with_most_orders\": \"data['customer_state'].mode()[0]\",\n",
        "            \"top_5_cities_by_orders\": \"data['customer_city'].value_counts().nlargest(5)\",\n",
        "            \"top_5_states_by_orders\": \"data['customer_state'].value_counts().nlargest(5)\",\n",
        "            \"top_3_states_by_sales\": \"data.groupby('customer_state')['price'].sum().nlargest(3)\",\n",
        "            \"sales_per_state\": \"data.groupby('customer_state')['price'].sum()\",\n",
        "            \"orders_by_state_and_city\": \"data.groupby(['customer_state', 'customer_city'])['order_status'].count().sort_values(ascending=False)\",\n",
        "        }\n",
        "\n",
        "        RECOMMENDATION_PROMPT_TEMPLATE = \\\"\\\"\\\"\n",
        "        You are an expert e-commerce marketing consultant. Your task is to provide a detailed, actionable, and comprehensive recommendation to a business owner based on a data-driven insight.\n",
        "\n",
        "        Insight from data analyst: {insight}\n",
        "\n",
        "        Detailed Recommendation:\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        INSIGHT_FORMATTING_PROMPT = \\\"\\\"\\\"\n",
        "        You are an expert data analyst. The user asked a question and you have the result of a data query.\n",
        "        Please format the raw result into a clear, professional, and conversational sentence.\n",
        "        Do not just print the numbers. Explain what they mean.\n",
        "        User Query: {query}\n",
        "        Raw Result: {raw_result}\n",
        "        Formatted Answer:\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        return llm, PANDAS_EXPRESSIONS, RECOMMENDATION_PROMPT_TEMPLATE, INSIGHT_FORMATTING_PROMPT\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}. Please try again or use a smaller model.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def run_query_with_llm(query, df, llm, expressions, insight_prompt, recommendation_prompt_template):\n",
        "    \\\"\\\"\\\"\n",
        "    Identifies the best pandas expression, executes it, and generates insight and recommendation.\n",
        "    \\\"\\\"\\\"\n",
        "    keyword_prompt = f\\\"\\\"\\\"\n",
        "    You are an expert data analyst. You are given a pandas DataFrame named 'data'.\n",
        "    Your task is to identify which of the following keywords best answers the user's query:\n",
        "    Keywords: {list(expressions.keys())}\n",
        "    Please provide only the single keyword that is the best match. Do not provide any other text or explanation.\n",
        "    Query: {query}\n",
        "    Response:\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        keyword = llm.invoke(keyword_prompt).strip()\n",
        "        if keyword in expressions:\n",
        "            expression_to_run = expressions[keyword]\n",
        "            raw_result = eval(expression_to_run, {'data': df, 'pd': pd})\n",
        "\n",
        "            insight_prompt_filled = insight_prompt.format(query=query, raw_result=raw_result)\n",
        "            insight = llm.invoke(insight_prompt_filled).strip()\n",
        "\n",
        "            recommendation_prompt_filled = recommendation_prompt_template.format(insight=insight)\n",
        "            recommendation = llm.invoke(recommendation_prompt_filled).strip()\n",
        "\n",
        "            return insight, recommendation\n",
        "        else:\n",
        "            return \"Sorry, I can't find an appropriate analysis for this question.\", \"\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\", \"\"\n",
        "\n",
        "# --- Streamlit App UI ---\n",
        "st.set_page_config(page_title=\"Analytics Chatbot\", page_icon=\"📊\", layout=\"wide\")\n",
        "st.title(\"📊 Analytics Chatbot\")\n",
        "st.write(\"Upload a CSV file and ask questions about your data.\")\n",
        "\n",
        "# --- Initialize Session State ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"data\" not in st.session_state:\n",
        "    st.session_state.data = None\n",
        "\n",
        "# --- File Upload Section ---\n",
        "st.header(\"Upload Your CSV File\")\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        st.session_state.data = pd.read_csv(uploaded_file)\n",
        "        st.success(\"File uploaded successfully! You can now start the chat below.\")\n",
        "        st.dataframe(st.session_state.data.head())\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file: {e}\")\n",
        "        st.session_state.data = None\n",
        "\n",
        "# --- Chat Interface ---\n",
        "if st.session_state.data is not None:\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"Start Chatting\")\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"What would you like to know about your data?\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Analyzing data...\"):\n",
        "                llm, PANDAS_EXPRESSIONS, RECOMMENDATION_PROMPT_TEMPLATE, INSIGHT_FORMATTING_PROMPT = get_llm_and_agent_components()\n",
        "                if llm:\n",
        "                    insight, recommendation = run_query_with_llm(\n",
        "                        prompt,\n",
        "                        st.session_state.data,\n",
        "                        llm,\n",
        "                        PANDAS_EXPRESSIONS,\n",
        "                        INSIGHT_FORMATTING_PROMPT,\n",
        "                        RECOMMENDATION_PROMPT_TEMPLATE\n",
        "                    )\n",
        "\n",
        "                    full_response = f\\\"\\\"\\\"**Insight:**\n",
        "{insight}\n",
        "\n",
        "**Recommendation:**\n",
        "{recommendation}\\\"\\\"\\\"\n",
        "                    st.markdown(full_response)\n",
        "                else:\n",
        "                    st.error(\"Failed to load the LLM model. Please check the model name or available resources.\")\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "else:\n",
        "    st.info(\"Please upload a CSV file to begin.\")\n",
        "\"\"\"\n",
        "\n",
        "# Second page: RAG Assistant App\n",
        "rag_app_content = \"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_community.docstore.document import Document\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# IMPORTANT: The provided API keys are not strictly needed as we are using a local LLM, but they might be\n",
        "# used for other components like Cohere Rerank.\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "\n",
        "# Define the model to use. Now using the larger 't5-xl' model.\n",
        "MODEL_NAME = \"google/flan-t5-xl\"\n",
        "\n",
        "# --- Centralized LLM and RAG Logic ---\n",
        "@st.cache_resource\n",
        "def get_rag_chain_components():\n",
        "    \\\"\\\"\\\"\n",
        "    Loads all components and assembles the RAG chain.\n",
        "    This function runs only once due to @st.cache_resource.\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        # Load the LLM Model and tokenizer (using the same model as the other app)\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=False\n",
        "            )\n",
        "            device_map = \"auto\"\n",
        "            llm_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                MODEL_NAME,\n",
        "                device_map=device_map,\n",
        "                quantization_config=quantization_config,\n",
        "            )\n",
        "            # Removed the device argument from the pipeline\n",
        "            llm_pipe = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_length=512)\n",
        "        else:\n",
        "            st.warning(\"CUDA is not available. Running the model on CPU may be very slow.\")\n",
        "            llm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "            llm_pipe = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_length=512, device=-1)\n",
        "\n",
        "        llm_instance = HuggingFacePipeline(pipeline=llm_pipe)\n",
        "\n",
        "        # Load the FAISS Index\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        faiss_index_path = \"faiss_index\"\n",
        "\n",
        "        if not os.path.exists(faiss_index_path):\n",
        "            st.warning(\"FAISS index not found. Creating a dummy index for demonstration.\")\n",
        "            docs = [Document(page_content=\"This is a document about a product.\", metadata={\"source\": \"dummy\"})]\n",
        "            faiss_index = FAISS.from_documents(docs, embeddings)\n",
        "            faiss_index.save_local(faiss_index_path)\n",
        "            # Re-load the index after creation\n",
        "            faiss_index = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "        else:\n",
        "            faiss_index = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "        # Create the Reranker\n",
        "        reranker = CohereRerank(cohere_api_key=COHERE_API_KEY, model=\"rerank-english-v3.0\", top_n=5)\n",
        "\n",
        "        # Create the Retriever with Re-ranking\n",
        "        base_retriever = faiss_index.as_retriever(search_kwargs={\"k\": 20})\n",
        "        compressed_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=reranker,\n",
        "            base_retriever=base_retriever\n",
        "        )\n",
        "\n",
        "        # Create the Memory and Prompt for a T5 model\n",
        "        prompt_template = \\\"\\\"\\\"You are an e-commerce assistant. Use the following context and chat history to answer the question.\n",
        "        If the answer is not in the provided documents, say \"I don't know the answer based on the provided context.\"\n",
        "\n",
        "        Chat History:\n",
        "        {chat_history}\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer:\\\"\\\"\\\"\n",
        "\n",
        "        memory = ConversationBufferWindowMemory(\n",
        "            k=3,\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        # Assemble the full Conversational RAG Chain\n",
        "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm_instance,\n",
        "            retriever=compressed_retriever,\n",
        "            memory=memory,\n",
        "            combine_docs_chain_kwargs={\n",
        "                \"prompt\": PromptTemplate(template=prompt_template, input_variables=[\"chat_history\", \"context\", \"question\"])\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return qa_chain\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading RAG components: {e}\")\n",
        "        st.warning(\"Please ensure all dependencies are installed and the FAISS index is available.\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "# --- Streamlit App UI ---\n",
        "# We call the cached function here to retrieve the RAG chain.\n",
        "rag_chain = get_rag_chain_components()\n",
        "st.set_page_config(page_title=\"E-Commerce Assistant\", page_icon=\"🛒\")\n",
        "st.title(\"🛒 E-Commerce Assistant\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"How can I help you?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = rag_chain.invoke({\"question\": prompt})\n",
        "            answer_text = response.get('answer', '')\n",
        "\n",
        "            cleaned_answer = \"\"\n",
        "            if \"Answer:\" in answer_text:\n",
        "                last_answer_index = answer_text.rfind(\"Answer:\")\n",
        "                cleaned_answer = answer_text[last_answer_index + len(\"Answer:\"):].strip()\n",
        "            else:\n",
        "                cleaned_answer = answer_text.strip()\n",
        "\n",
        "            st.markdown(cleaned_answer)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": cleaned_answer})\n",
        "\"\"\"\n",
        "\n",
        "# --- 2. Create the file structure ---\n",
        "print(\"Creating file structure...\")\n",
        "# Remove the old 'pages' directory if it exists to ensure a clean start\n",
        "if os.path.exists(\"pages\"):\n",
        "    shutil.rmtree(\"pages\")\n",
        "\n",
        "# Create the main app file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(main_app_content)\n",
        "\n",
        "# Create the 'pages' directory\n",
        "pages_dir = \"pages\"\n",
        "os.makedirs(pages_dir, exist_ok=True)\n",
        "\n",
        "# Create the first page file\n",
        "with open(os.path.join(pages_dir, \"1_Analytics_App.py\"), \"w\") as f:\n",
        "    f.write(analytics_app_content)\n",
        "\n",
        "# Create the second page file\n",
        "with open(os.path.join(pages_dir, \"2_RAG_Assistant_App.py\"), \"w\") as f:\n",
        "    f.write(rag_app_content)\n",
        "\n",
        "print(\"File structure created successfully.\")\n",
        "\n",
        "# --- 3. Run the Streamlit app with ngrok ---\n",
        "def run_streamlit_app():\n",
        "    # Kill all existing ngrok processes before starting a new one\n",
        "    print(\"Killing any existing ngrok processes...\")\n",
        "    try:\n",
        "        ngrok.kill()\n",
        "    except Exception as e:\n",
        "        print(f\"No ngrok processes to kill or an error occurred: {e}\")\n",
        "\n",
        "    # Start streamlit app in background\n",
        "    print(\"Starting Streamlit app...\")\n",
        "    proc = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "    # Wait for the Streamlit app to start\n",
        "    print(\"Waiting for Streamlit app to start...\")\n",
        "    timeout = 180\n",
        "    start_time = time.time()\n",
        "\n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:8501\")\n",
        "            if response.status_code == 200:\n",
        "                print(\"Streamlit app is running!\")\n",
        "                break\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\"Streamlit not ready yet. Retrying in 5 seconds...\")\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        print(\"Error: Streamlit app failed to start within the timeout period.\")\n",
        "        proc.kill()\n",
        "        return\n",
        "\n",
        "    # Open ngrok tunnel\n",
        "    try:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "        print(f\"Your Streamlit app is running at: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start ngrok tunnel: {e}\")\n",
        "        ngrok.kill()\n",
        "        proc.kill()\n",
        "        raise\n",
        "\n",
        "# Call the function to run the app\n",
        "run_streamlit_app()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-14T10:36:23.784117Z",
          "iopub.execute_input": "2025-08-14T10:36:23.784717Z",
          "iopub.status.idle": "2025-08-14T10:36:30.396709Z",
          "shell.execute_reply.started": "2025-08-14T10:36:23.784685Z",
          "shell.execute_reply": "2025-08-14T10:36:30.396075Z"
        },
        "id": "WQexOFgLkRwd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "y3BqN0VKkRwd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thanks"
      ],
      "metadata": {
        "id": "zaOMQD7nkRwe"
      }
    }
  ]
}